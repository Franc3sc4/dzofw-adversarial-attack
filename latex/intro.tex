\section{Introduction}
The main goal of this report is to analyze three different Strochastic Gradient Free Frank-Wolfe algorithms for producing universal adversarial perturbations. These perturbations are designed to fool the LeNet5 convolutional neural network considering the classification task on the MNIST dataset.\\
Before discussing the implementation, some key concepts about the adversarial attacks and the Frank-Wolfe framework are introduced. The last two sections concern the experiments and the conclusive comparison of the results.

\subsection{Adversarial Attacks}
An adversarial attack is a machine learning technique that has the aim of fooling a classifier by providing it with
carefully designed inputs, called \textit{adversarial examples}. An adversarial example is a datapoint that has
been perturbed or distorted in order to cause the classifier to make an erroneous prediction.

Adversarial attacks can be divided into targeted and untargeted attacks. \textit{Targeted} attacks have the aim to
misclassify the input image to a target class, while \textit{untargeted} attacks are aimed at only misclassifying
the input image to any other class except the true class.

A further way to categorize adversarial attacks is to divide them into white-box and black-box attacks, based on
the information accessible to the attacker. To perform a \textit{white-box} attack, the classifier, the trained
parameter values, and the analytical form of the classifier loss function must be all available to the attacker. On
the contrary, in a \textit{black-box} setting, only a zeroth-order oracle is accessible, that is for an input $(\mathbf{x},y)$,
the value of the loss function $F(\mathbf{x},y)$ is accessible. Therefore, black-box attacks can be seen as an optimization problem,
where the adversarial attack is formulated as a maximization of the classification loss function, whose values are
accessible only via a zeroth-order oracle.

This kind of adversarial attacks can be further categorized into score-based and decision-based attacks.
In \textit{score-based} attacks, the attacker directly observes a loss value, class probability, or some other
continuous output of the classifier on a given example, whereas in \textit{decision-based} attacks, the attacker gets to
observe only the hard label predicted by the classifier.

% For example, an adversarial example would be a perturbed image of a pig which would still be a pig to a human eye, while
% the classifier would now output the class to be that of an airplane.

% However, in most real-world deployments, it is impractical to assume complete access to the classifier
% and analytic form of the corresponding loss function, which makes black-box settings more realistic.

% Black-box adversarial attacks can be broadly categorized across a few different dimensions: optimization-based
% versus transfer-based attacks, and score-based versus decision-based attacks.
% The zeroth-order information can be further categorized into score-based and decision-based attacks.

In this report we will focus on untargeted, black-box, score-based adversarial attacks.

\subsection{Universal Adversarial Perturbations}
Universal perturbations are small perturbations that, when applied to the input images, are able
to fool a state-of-the-art deep neural network classifier. These perturbations are quasi-imperceptible to human eyes,
due to their small norm, and therefore they are difficult to detect. However, what makes universal perturbations special,
is their capability to generalize well both on a large dataset and across different deep neural network architectures.
In fact, an important property of universal perturbations is that they are image-agnostic. This means that
they don't depend on a single image, but rather they are able to cause label estimation change for most of
the images contained in a dataset.

It is important to notice that there's no unique universal perturbation: different random shuffling of the data used
to compute the perturbation can lead to a diverse set of universal perturbations.

Furthermore, universal perturbations mostly make natural images classified with specific labels, called
\textit{dominant labels}. These dominant labels are not determined a priori, but rather they are automatically
found by the algorithm that computes the universal perturbation. In paper \textcolor{red}{XXX}, the authors
hypothesize that "these dominant labels occupy large regions in the image space, and therefore represent good
candidate labels for fooling most natural images".

Finally, although some fine-tuning strategies can be adopted to make deep neural networks more robust to adversarial
attacks, they are not sufficient to make the classifiers immune to the attacks. In fact, the fine-tuning of a classifier
with universal perturbations leads to an improvement in the classification of perturbed images, however it is always
possible to find a small universal perturbation that can fool the network.

% non-convex problem

% the objective is not to find the smallest perturbation that fools most of the data points,
% but rather to find one such perturbation with sufficiently small norm

% universal perturbations exploit some geometric correlations between different parts in the decision
% boundary of the classifier

