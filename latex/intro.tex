\section{Introduction}
The content of this report is mainly based on the analysis of three papers:
in \cite{A2} is introduced the definition of Universal Adversarial Perturbations, while in \cite{A3} and
\cite{A4} are proposed different zeroth-order Frank-Wolfe algorithms that can be used to generate such perturbations. In particular, we carried out some experiments 
on the basis of the applications described in \cite{A3}, section VIII.B.
In the following subsections, we start by explaining some of the key concepts that are necessary to understand our work.

\subsection{Adversarial Attacks}
An adversarial attack is a machine learning technique that has the aim of fooling a classifier by providing it some
carefully designed inputs, called \textit{adversarial examples}. An adversarial example is a datapoint that has
been perturbed or distorted so that the classifier makes an erroneous prediction.

Adversarial attacks can be divided into targeted and untargeted attacks. The primer ones aim to
misclassify the input image to a target class, while the latter ones aim to only misclassify
the input image with any other class except the true class.

A further distinction is between white-box and black-box attacks, based on
the information accessible to the attacker. A \textit{white-box} attack involves the knowledge about the classifier, the trained
parameter values and the analytical form of the classifier's loss function. On
the contrary, in a \textit{black-box} setting, only a zeroth-order oracle is accessible, i.e. only the values of the loss function are available.

Black-box adversarial attacks can be further categorized into score-based and decision-based attacks.
In \textit{score-based} attacks, the attacker directly observes a loss value, class probability, or some other
continuous output of the classifier on a given example, whereas in \textit{decision-based} attacks, the attacker
observes only the hard label predicted by the classifier.

% For example, an adversarial example would be a perturbed image of a pig which would still be a pig to a human eye, while
% the classifier would now output the class to be that of an airplane.

% However, in most real-world deployments, it is impractical to assume complete access to the classifier
% and analytic form of the corresponding loss function, which makes black-box settings more realistic.

% Black-box adversarial attacks can be broadly categorized across a few different dimensions: optimization-based
% versus transfer-based attacks, and score-based versus decision-based attacks.
% The zeroth-order information can be further categorized into score-based and decision-based attacks.

In this report we will focus on untargeted, black-box, score-based adversarial attacks.

\subsection{Universal Adversarial Perturbations}
\label{section:perturb}
Universal Adversarial Perturbations are small perturbations that, when applied to the input images, are able
to fool a state-of-the-art deep neural network classifier. These perturbations are quasi-imperceptible to human eyes,
due to their small norm, and therefore they are difficult to detect. However, what makes Universal Adversarial Perturbations special,
is their capability to generalize well both on a large dataset and across different deep neural network architectures.
In fact, an important property of universal perturbations is that they are image-agnostic. This means that
they don't depend on a single image, but rather they are able to cause label estimation change for most of
the images contained in a dataset.

It is important to notice that there is no unique Universal Adversarial Perturbation: different random shuffling of the data used
for the computation can lead to a diverse set of perturbations.

Furthermore, these kinds of perturbations mostly make natural images classified with specific labels, called
\textit{dominant labels}. These dominant labels are not determined a priori, but rather they are automatically
found by the algorithm that computes the universal perturbation. In paper \cite{A2}, the authors
hypothesize that <<these dominant labels occupy large regions in the image space, and therefore represent good
candidate labels for fooling most natural images>>. We will discuss this axpect in the Experiment and Conclusion sections.

Finally, although some fine-tuning strategies can be adopted to make the deep neural networks more robust to Adversarial
Attacks, they are not sufficient to make the classifiers totally immune. In fact, + it is always
possible to find a small Universal Adversarial Perturbation that can fool the network.

% non-convex problem

% the objective is not to find the smallest perturbation that fools most of the data points,
% but rather to find one such perturbation with sufficiently small norm

% universal perturbations exploit some geometric correlations between different parts in the decision
% boundary of the classifier

