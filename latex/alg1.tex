\subsection{Decentralized Stochastic Gradient Free Frank-Wolfe}
In this section the goal is to solve a special case of (1), that is
\begin{equation}
	\mbox{\boldmath$ \delta$}' = \argmin_{\lVert \mbox{\boldmath$\scriptstyle \delta$}\rVert_{\infty}\leq\varepsilon} \frac{1}{M}\sum_{i=1}^M \mathbb{E}_{(\mathbf{x},y)\sim \mathcal{P}_i}[- F_i(\mathbf{x}+\mbox{\boldmath$ \delta$},y)].
\end{equation}
The decentralized architecture is composed of a central node, called master node, and other nodes connected to it but not to each other, called workers.
The workers are connected to the master node to read, write and exchange information.\\
In the Decentralized Stochastic Gradient Free Frank-Wolfe method implemented in Algorithm 1, the data is spread to $M$ workers and the starting point \mbox{\boldmath$ \delta$}$_{0}$ is initialized to a null $d$-vector.\\
Each worker $i$ computes its local gradient using the I-RDSA scheme and considering a batch of images $\mathbf{x}_i$ and the corresponding labels $\mathbf{y}_i$ from the dataset. Then the workers apply the smoothing scheme (3).\\
When all the workers have finished their tasks, they send their results to the master node, which computes the average of the estimated gradients and send back the new iterate \mbox{\boldmath$ \delta$}$_{t+1}$, obtained from (5) and (6) with $\gamma_t = \frac{2}{t+8}$.\\
When all of the iterations are done, the algorithm returns the hystory of all the perturbations computed by the master node.

\begin{algorithm}
	\caption{Decentralized SGF FW}\label{decentralized} 
	 \textbf{Input} Batches of images $\{\mathbf{x}_i\}_{i=1}^M$, batches of labels $\{\mathbf{y}_i\}_{i=1}^M$, loss function $F$, number of queries $T$, number of workers $M$, image dimension $d$, tolerance $\varepsilon$, number of sampled directions $m$.\\
	 \textbf{Output} Universal perturbation's history.
	\begin{algorithmic}[1]		
		\State Initialize \mbox{\boldmath$ \delta$}$_{0} = \mathbf{0}$.
		\For {$t = 0, \dots, T-1$}
		\State Master node computes parameters required for the computation of the I-RDSA scheme: 
		{\scriptsize\[(\rho_t,c_t)_{I-RDSA} =\bigg(\frac{4}{\big(1+\frac{d}{m}\big)^{1/3}(t+8)^{2/3}}, \frac{2\sqrt{m}}{d^{3/2}(t+8)^{1/3}}\bigg)\]}
		\State Each worker $i$ computes the I-RDSA scheme:\newline Sample $\{\mathbf{z}_{j,t}\}_{j=1}^m \sim\mathcal{N}(0,\mathbb{1}_d)$ \newline
		 \[\mathbf{g}_i = \frac{1}{m} \sum_{j=1}^{m} \frac{F(\mathbf{x}_i + \mbox{\boldmath$ \delta$}_t + c_t\mathbf{z}_{j,t}, \mathbf{y}_i) - F(\mathbf{x}_i + \mbox{\scriptsize\boldmath$ \delta$}_t, \mathbf{y}_i)}{c_t}\mathbf{z}_{j,t}\]
		
		\State Each worker $i$ computes the gradient smoothing scheme:  \[\mathbf{g}_{i,t}= (1- \rho_t)\mathbf{g}_{i,t-1} + \rho_t\mathbf{g}_i.\]
		\State Each worker $i$ pushes $\mathbf{g}_{i,t}$ to the master node.
		\State Master node computes the gradients average:
		\[\mathbf{g}_t = \frac{1}{M} \sum_{i=1}^{M} \mathbf{g}_{i,t}.\]
		\State Master node computes $\mathbf{v}_t = - \varepsilon sign(\mathbf{g}_t)$.
		\State Master node computes \mbox{\boldmath$ \delta$}$_{t+1} = (1-\gamma_t)\mbox{\boldmath$ \delta$}_t + \gamma_t\mathbf{v}_t$ and sends it to all nodes.
		\EndFor

	\end{algorithmic}
\end{algorithm}
