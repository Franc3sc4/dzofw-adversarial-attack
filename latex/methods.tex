\section{Methods}
Recently, decentralized and distributed settings are getting significant attention due to the need to exploit data
parallelization in case of complex models trained on huge datasets, that typically require a storage that exceeds
a machine capacity. In brief, in decentralized setups the devices (or workers) exchange information with a master node, while in distributed setups the devices are connected in a peer-to-peer manner and therefore exchange information only with their neighbors. \\
\indent The following optimization algorithms are designed to accommodate distributed data across multiple devices.\\
\indent All the proposed algorithms will consider as iterates the perturbations \mbox{\boldmath$ \delta$}$_t$ converging to the solution of the optimization problem (1). In particular, the iterates $\mathbf{x}_t$ in the previously defined gradient approximation schemes have to be intended as a batch of dataset images $\mathbf{x}$ to which the t-th perturbation \mbox{\boldmath$ \delta$}$_t$ has been applied, i.e. $\mathbf{x}_t = \mathbf{x} + \mbox{\boldmath$ \delta$}_t$ are the perturbed data.