\section{Gradient Free Frank Wolfe}
Gradient free optimization algorithms find application in settings where the explicit closed form of the loss function is not available or the gradient evaluation is computationally prohibitive. A prime example is black-box adversarial attacks on neural networks, where only the model output is known while the architecture and weights remain unknown. In fact, gradient free methods exploits just zeroth-order oracle calls (i.e. loss function evaluations) to solve an optimization problem.\\
In particular, this report focuses on zeroth-order Frank Wolfe algorithms for constrained optimization problems. Unlike the Projected Gradient method that requires expensive projection operations, the Frank Wolfe framework provides computational semplicity by making use of instances of linear minimization. \\
Considering the application of interest, that is black-box adversarial attacks, a "good enough" feasible solution is often adequate. Therefore, the use of biased gradient estimates obtained from zeroth-order information is suitable for performing this task.\\

1) SPIEGARE ZEROTH-ORDER OPTIMIZATION + STOCHASTIC AND CONSTRAINED OPTIMIZATION\\
2) SPIEGARE FW FROM 1ST ORDER TO 0TH ORDER\\
3) INTRODURRE IL CONCETTO DI DECENTRALIZED E DISTRIBUTED SETTINGS\\

